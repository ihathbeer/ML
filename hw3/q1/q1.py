# Question 1 HW3
# Author: Andrew Nedea

# 4 classes with uniform priors, Gaussian pdfs
import numpy as np
from numpy.linalg import eig
from scipy.stats import multivariate_normal
from collections import defaultdict
import matplotlib.pyplot as plt
import matplotlib
from keras import models
from keras import layers
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
import tensorflow as tf
import random
import math
import logging
import pickle
import random
from sklearn.metrics import accuracy_score

# Disable TF warnings (temp. measure)
tf.get_logger().setLevel(logging.ERROR)
# Seed pseudo-random no. generator
random.seed()

# Settings
LOAD_CACHED_DATA = True
SOLVE = False
OBSERVE = True

# number of splits for cross-validation
K_SPLIT = 10

# Paths to various pickles
SIZE_PERCEPT_NO_ERROR_PATH = 'size_to_perceptron_no_error.pickle'
# training set size to optimal no. of perceptrons for that model
SIZE_TO_OPT_P_PATH = 'size_to_optimal_p.pickle' 
MINP_TEST_ERROR_PATH = 'minp_test_error.pickle'
SIZE_TO_TEST_ERROR_PATH = 'size_to_test_error.pickle'
TRAIN_DATA_PATH = 'train.pickle'
TEST_DATA_PATH = 'test.pickle'

# Eigenvalues of covariance matrices should be in range [0.5, 1.4]
COV_EIGEN_BOUND = [0.5, 1.4]
# Set sizes
TRAIN_SET_SIZES = [100, 200, 500, 1000, 2000, 5000]
TEST_SET_SIZES = [100000]

# Maximum number of perceptrons to try out for a ANN MLP
MAX_PERCEPTRON_NO = 15
# Number of features to each sample
FEATURE_NO = 3
# No. of epochs to train ANN MLPs
EPOCH_NO = 500
# Where data is saved
DATA_PATH = 'data/'
IMG_PATH = 'pics/'

# Class parameters
CLASS_CONFIG = {
        1: { 
            "mean": [1, 1, 1],
            "cov": [[ 1.01, 0.03, 0.02 ],
                    [ 0.03, 1.12, 0.03],
                    [ 0.02, 0.03, 1.06]],
            "color": "green",
            "marker": "o"
        },
        2: { 
            "mean": [1, -1, 1],
            "cov": [[ 0.92, 0.01, 0.09 ],
                    [ 0.01, 1.02, 0.03],
                    [ 0.09, 0.03, 0.66]],
            "color": "red",
            "marker": "^",
        },
        3: { 
            "mean": [-1, -1, 1],
            "cov": [[ 1.09, 0.02, 0.19 ],
                    [ 0.02, 1.01, 0.03],
                    [ 0.19, 0.03, 0.61]],
            "color": "blue",
            "marker": "s"
        },
        4: { 
            "mean": [-1, 1, -1],
            "cov": [[ 1.02, 0.03, 0.24 ],
                    [ 0.03, 1.27, 0.2 ],
                    [ 0.24, 0.2, 0.73]],
            "color": "purple",
            "marker": "D"
        },
};

# Uniform prior
UCLASS_PRIOR = 1.0 / len(CLASS_CONFIG)

def generate_and_sample(sample_no) -> dict:
    """
    Generates distributions and samples for each class and returns a dictionary 
    that maps class labels to a child dictionary. The child dictionary contains 
    two keys (samples and dist). The "samples" key maps to the vector of samples
    X for that class and the "dist" key maps to the multivariate_normal object for
    the class.

    i.e.
    {
        1: {"samples": [[12, 34, 42], ... [123, 2, 3]], "dist": object},
        2: ..
    }

    :param sample_no: number of samples to generate
    """
    results = {}

    sample_no_per_class = int(sample_no / len(CLASS_CONFIG))

    print(f'Generating {sample_no_per_class} samples per class')

    for label, params in CLASS_CONFIG.items():
        dist = multivariate_normal(params["mean"], params["cov"])
        x = dist.rvs(size=sample_no_per_class)

        results[label] = {"samples": x, "dist": dist}

    return results

def covariance_contract() -> None:
    """
    Ensures that the covariance contract is respected (that the eigenvalues of the
    covariance matrices are within COV_EIGEN_BOUND).
    """
    for label, params in CLASS_CONFIG.items():
        if not all([(COV_EIGEN_BOUND[0] <= eigenv <= COV_EIGEN_BOUND[1]) \
                for eigenv in eig(params["cov"])[0]]):
            raise Exception('At least one covariance matrix has eigenvalues outside interval!')
        print(eig(params["cov"])[0])

def plot_samples(class_dist_samples: dict, title: str) -> None:
    """
    Plots the sample points generated by generate_and_sample.

    :param class_dist_samples: dictionary with class labels for keys and child dictionary containing
                               "samples" for values
    """
    fig = plt.figure(int(random.randrange(0, 999999)))
    ax = fig.add_subplot(1, 1, 1, projection='3d')
    ax.title.set_text(title)
    ax.set_xlabel('x0')
    ax.set_ylabel('x1')
    ax.set_zlabel('x2')

    handles = []
    for class_label, data in class_dist_samples.items():
        samples = data["samples"]
        img = ax.scatter(samples[:, 0], samples[:, 1], samples[:, 2],
                c=CLASS_CONFIG[class_label]["color"], label=f'Class {class_label}')
        handles.append(img)

    plt.legend(handles=handles)
    plt.savefig(f'{IMG_PATH}/{title}.png', bbox_inches='tight', pad_inches=0)
    # plt.show()

def generate_sets() -> (dict, dict):
    """
    Generates training and testing sets for each size specified in
    TRAIN_SET_SIZES and TEST_SET_SIZES, respectively.

    For each type of set (train & test), a dictionary is created indexed
    by set size with value equal to the dict returned by generate_and_sample for
    that size.

    example of object returned:
           
    tuple(
          v----- training set of size 100 for each class (1, 2 etc.)
        {100: {1: {"samples": [[..]], "dist": }, 2: {"samples: [[..]], "dist": }, ..},
        {10000: {1: {"samples: [..], "dict": ..}, 2: {"samples":...}}}
         ^--- test set of size 10000 for each class (1, 2 etc.)
    )

    :return: a tuple made up of training set dict and test set dict
    """
    covariance_contract()
    # maps from set size to dict returned by generate_and_sample of that size
    train_sets = {}
    test_sets = {}

    print('Generating sets..')

    for size in TRAIN_SET_SIZES:
        train_sets[size] = generate_and_sample(size)
        print(f' -> Generated training set of size {size}')

    for size in TEST_SET_SIZES:
        test_sets[size] = generate_and_sample(size)
        print(f' -> Generated test set of size {size}')

    return train_sets, test_sets

def save_dict(dict_:dict, filename: str) -> None:
    """
    Saves given dict to {DATA_PATH}/{filename} as pickle.

    :param dict_: dict to save
    :param filename: filename to save as
    """
    with open(f'{DATA_PATH}/{filename}', 'wb') as handle:
        pickle.dump(dict_, handle, protocol=pickle.HIGHEST_PROTOCOL)
        print(f'Saved dict to {filename}')

def load_dict(filename: str) -> dict:
    """
    Loads pickled dict by filename.

    :param filename: name of pickle to retrieve
    :return: dict object
    """
    with open(f'{DATA_PATH}/{filename}', 'rb') as fh:
        obj = pickle.load(fh)
        return obj

    print(f'Error, could not read sets from {filename}!')

def minp_classification(class_dist_samples: dict):
    """
    Takes in a dictionary that maps a class label to samples & the distribution
    the samples came from. It then performs min-p classification to determine
    the probability of error.

    :param class_dist_samples: dict returned by generate_and_sample
    """
    labels = list(class_dist_samples.keys())
    incorrectly_labeled_no = 0

    # maps class label to distributions
    distributions = {}
    for label, items in class_dist_samples.items():
        distributions[label] = items["dist"]

    # predicted label -> (true_label, sample)
    classifications = defaultdict(list)
    counter = 0
    print('Total sample no.: ', len(class_dist_samples) * len(class_dist_samples[1]['samples']))
    # for each class
    for true_label, items in class_dist_samples.items():
        # for each sample of that class
        for sample in items["samples"]:
            best_prob = -999999
            best_label = 0
            # for each class type determine posterior
            for label in labels:
                prob = distributions[label].pdf(sample) * UCLASS_PRIOR
                
                # check if this risk beats previous
                if best_prob < prob:
                    best_prob = prob
                    best_label = label

            classifications[best_label].append((true_label, sample))

            # increment counter
            counter += 1

            # print where we are
            if counter % 10000 == 0:
                print('Counter: ', counter)


    # correctly classified samples
    cc = []
    # incorrectly classified samples
    icc = []
    # for each class / label and samples classified as such
    for classification, labeled_samples in classifications.items():
        # for each sample classified as 'classification'
        for (true_label, sample) in labeled_samples:
            # check if incorrectly classified
            if true_label != classification:
                #print(f'Correct label {true_label} Classified as {classification}')
                incorrectly_labeled_no += 1
                icc.append((true_label, sample))
            else:
                cc.append((true_label, sample))

    # Calculate error
    error = incorrectly_labeled_no / counter

    print('Incorrectly classified no.: ', incorrectly_labeled_no)
    print('P(error)', error)

    save_dict({'e': error}, MINP_TEST_ERROR_PATH)

    return cc, icc, error

def plot_classification(cc: list, icc: list, title: str):
    """
    Plots correctly and incorrectly classified samples.

    :param cc: list of correctly classified sample tuples (true_label, sample)
    :param icc: list of incorrectly classified sample tuples (true_label, sample)
    :param title: what to name plot
    """
    # dict to hold correctly classified samples by class
    ccl = defaultdict(list)
    # dict to hold incorrectly classified samples by class
    iccl = defaultdict(list)

    # convert to dicts indexed by true class label
    for true_label, sample in cc:
        ccl[true_label].append(sample)

    for true_label, sample in icc:
        iccl[true_label].append(sample)

    fig = plt.figure(1)
    ax = fig.add_subplot(1, 1, 1, projection='3d')
    ax.title.set_text(f'Classification result: {title}')
    ax.set_xlabel('x0')
    ax.set_ylabel('x1')
    ax.set_zlabel('x2')

    handles = []

    for k in ccl.keys():
        # convert each entry to an np array for slicing
        ccl[k] = np.array(ccl[k])
        iccl[k] = np.array(iccl[k])

        handles.append(ax.scatter(ccl[k][:, 0], ccl[k][:, 1], ccl[k][:, 2],
            marker=CLASS_CONFIG[k]['marker'], color='green',
            label=f'Class {k} correctly classified'))

        handles.append(ax.scatter(iccl[k][:, 0], iccl[k][:, 1], iccl[k][:, 2],
            marker=CLASS_CONFIG[k]['marker'], color='red',
            label=f'Class {k} incorrectly classified'))

    plt.legend(handles=handles)
    plt.show()

def create_nn(perceptron_no: int):
    """
    Creates a multi-layer perceptrons with the specified number of perceptrons
    for its first layer. Its second layer uses a softmax activation function to fit
    results within [0, 1].

    :param perceptron_no: number of perceptrons:
    :return: compiled model
    """
    net = models.Sequential()
    # add hidden layer
    net.add(layers.Dense(units=perceptron_no, activation='elu', input_shape=(FEATURE_NO,)))
    # add output layer
    net.add(layers.Dense(units=len(CLASS_CONFIG), activation='softmax'))
    # compile
    net.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])

    return net

def create_classifier(perceptron_no: int, epoch_no: int=500):
    """
    Creates and returns classifier based on a MLP with given number of perceptrons
    and trained given number of times.

    :param perceptron_no: number of perceptrons
    :param epoch_no: number of epochs to train ANN MLP
    :return: KerasClassifier object configured to spec
    """
    nn = KerasClassifier(build_fn=create_nn, perceptron_no=perceptron_no, epochs=epoch_no, batch_size=100, verbose=0)

    return nn

def run_cross_val(X, Y, perceptron_no, epoch_no):
    """
    Runs 10-fold cross-validation on given training data.

    :param X: data
    :param Y: labels
    :param perceptron_no: number of perceptrons of NN
    :param epoch_no: number of epochs to train on
    :return: mean accuracy obtained from the k-folds
    """
    print(f'Sample no: {len(Y)} {len(X)}')

    nn = create_classifier(perceptron_no, epoch_no)
    scores = cross_val_score(nn, X, Y, cv=K_SPLIT)

    return np.mean(scores)

def splice(set_: dict) -> (list, list):
    """
    Processes data for training.

    :param set_: dict indexed by class label that maps to a dict with two fields
                 (samples and distr), as returned by generate_sets

    :return: tuple containing the features and their corresponding labels
    """
    X = []
    Y = []

    for label, items in set_.items():
        for sample in items["samples"]:
            X.append(np.array(sample))
            Y.append(label)

    X = np.array(X)
    Y = np.array(Y)

    # one-hot encode label
    Y = Y.reshape(len(Y), 1)
    onehot_encoder = OneHotEncoder(sparse=False)
    Y = onehot_encoder.fit_transform(Y)

    return X, Y

# ANN stuff
def find_best_config(train_sets):
    """
    Finds the best number of perceptron for each set size.

    :param train_sets: sets to find best config for
    :return: a dict mapping set size to the best model for that size trained
             on the set with that size
    """
    # maps set size -> optimal perceptron no.
    size_to_optimal_p = {}
    # maps set size -> list of pair of (perceptron no, prob. of error)
    size_to_perceptron_error = defaultdict(list)
    # maps set size -> pair of (X, Y)
    size_to_XY = {}

    # Run cross-validation to determine best config (no. of perceptrons)
    # for each set size
    for size, set_ in train_sets.items():
        print(f'=== Training set of size {size} ===')
        best_error = 9999
        best_perceptron_no = 0

        # get training pair
        X, Y = splice(set_)
        # cache training pair
        size_to_XY[size] = (X, Y)

        # try out different no. of perceptrons
        for k in range(1, MAX_PERCEPTRON_NO + 1):
            # get mean accuracy from cross-validation
            acc = run_cross_val(X, Y, k, EPOCH_NO)
            # compute error
            error = 1 - acc

            # save the error for this config for this size
            size_to_perceptron_error[size].append((k, error))
            print(f'Perceptron no. = {k} Avg cross-validation acc: {acc}')

            # check if this configuration beats the running best error-wise
            if error < best_error:
                best_error = error
                best_perceptron_no = k

        print(f'Optimal no. of perceptrons for set size {size}: {best_perceptron_no}')
        print('\n')

        # save best config (no. of perceptrons) for this size set
        size_to_optimal_p[size] = best_perceptron_no

    # Save size_to_perceptron_error and size_to_optimal_p to file
    save_dict(size_to_perceptron_error, SIZE_PERCEPT_NO_ERROR_PATH)
    save_dict(size_to_optimal_p, SIZE_TO_OPT_P_PATH)

    # Reconstruct optimal models & train them on (whole) training set pertaining to their size
    size_to_model = {}

    for size, optimal_p in size_to_optimal_p.items():
        # rebuild model with same no. of perceptrons it achieved best accuracy on
        size_to_model[size] = create_classifier(optimal_p)
        # train model
        size_to_model[size].fit(size_to_XY[size][0], size_to_XY[size][1])

    return size_to_model

def solve(train_sets, test_sets):
    """
    Finds the best config (no. of perceptrons) for each training set by
    picking the ones with the lowest error / highest accuracy. It then runs
    each trained optimal model (each model corresponding to a training set size) 
    on the test set and reports the errors.
    """
    size_to_model = find_best_config(train_sets)
    X_test, Y_test = splice(test_sets[100000])
    size_to_error = {}

    for size, model in size_to_model.items():
        # run on test set
        prediction = model.predict_proba(X_test)

        incorrectly_classified_no = 0
        error = 0

        for k in range(len(Y_test)):
            predicted_posteriors = prediction[k]
            # retrieve index of predicted class label by picking the maximum poster
            predicted_index = np.argmax(predicted_posteriors)

            # retrieve index of actual label
            actual_index = np.argmax(Y_test[k])
            # check if mismatch between predicted label & actual label
            if predicted_index != actual_index:
                incorrectly_classified_no += 1

        # determine error for this MLP model
        error = incorrectly_classified_no / len(Y_test)

        print(f'Min. prob of error (Ntrain = {size}) = {error}')
        size_to_error[size] = error

    save_dict(size_to_error, SIZE_TO_TEST_ERROR_PATH)

def print_dict(dict_):
    """
    Prints a nice key->val representation of given dict.

    :param dict_: dict to print
    """
    [print(k, ' -> ', value) for k, value in dict_.items()]

def plot_error_vs_size():
    """
    Plots the performance achieved by the min-p classifier and each MLP
    model.
    """
    minp_test_error = load_dict(MINP_TEST_ERROR_PATH)
    size_to_test_error = load_dict(SIZE_TO_TEST_ERROR_PATH)

    print_dict(minp_test_error)
    print_dict(size_to_test_error)

    # Get heuristic data points
    x = list(size_to_test_error.keys())
    y = list(size_to_test_error.values())

    # List of handles
    handles = []
    # Create figure
    fig = plt.figure()
    ax = fig.add_subplot(1, 1, 1)
    # Create scatter of empirically determined errors
    empirical_dp = ax.scatter(x, y, label=f'Empirical errors', color='green', s=40)
    handles.append(empirical_dp)
    # Add labels
    ax.title.set_text('Error on test set vs Training set size')
    ax.set_xscale('log')
    ax.set_xticks(x)
    ax.get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())
    ax.set_xlabel('Training set size')
    ax.set_ylabel('Prob. of error')

    # Generate line for min-p error (aspirational error)
    optimal_dp = ax.axhline(y=minp_test_error['e'], color='red', label=f'Ideal error',
            dashes=[2, 2])
    handles.append(optimal_dp)
    # Set Y-limit to contain optimal error
    plt.ylim(minp_test_error['e']*0.95, max(y)*1.05)
    plt.legend(handles=handles)
    plt.savefig(f'{IMG_PATH}/size_to_test_error.png', bbox_inches='tight', pad_inches=0.2)
    plt.show()

def plot_optimal_perceptron_no_vs_size():
    """
    Plots the optimal number of perceptrons vs the size of the training set.
    """
    size_to_optimal_p = load_dict(SIZE_TO_OPT_P_PATH)

    # Extract x, y
    x = list(size_to_optimal_p.keys())
    y = list(size_to_optimal_p.values())
    
    # Create figure
    fig = plt.figure()
    ax = fig.add_subplot(1, 1, 1)
    # Add labels
    ax.title.set_text('Optimal no. of perceptrons vs Training set size')
    ax.set_xlabel('Training set size')
    ax.set_xscale('log')
    ax.set_ylabel('No. of perceptrons')
    # Create scatter
    ax.scatter(x, y)
    plt.show()

def plot_error_vs_perceptron_no():
    """
    Plots the error achieved by each configuration (no. of perceptrons) for
    each given set size.
    """
    size_p_no_error = load_dict(SIZE_PERCEPT_NO_ERROR_PATH)
    # Configure layout
    count = 1

    for size, data in size_p_no_error.items():
        fig = plt.figure(count)
        # Unpack data
        percept_no = [entry[0] for entry in data]
        error = [entry[1] for entry in data]
        # Create plot
        ax = fig.add_subplot(1, 1, 1)
        ax.title.set_text(f'N={size}')
        ax.set_xlabel('No. of perceptrons')
        ax.set_ylabel('Prob. of error')
        ax.stem(percept_no, error, '-.')
        # Increment plot count
        count += 1
        plt.savefig(f'{IMG_PATH}/error_vs_p_no_{size}.png', bbox_inches='tight', pad_inches=0.2)
    
    #plt.show()

def main():
    if not LOAD_CACHED_DATA:
        train_sets, test_sets = generate_sets()
        save_dict(train_sets, TRAIN_DATA_PATH)
        save_dict(test_sets, TEST_DATA_PATH)
    else:
        print('Using cached data')
        # Load already-generated sets to save time
        train_sets = load_dict(TRAIN_DATA_PATH)
        test_sets = load_dict(TEST_DATA_PATH)

    if SOLVE:
        # Min-p classification
        cc, icc, error = minp_classification(test_sets[100000])

        if OBSERVE:
            plot_classification(cc, icc, 'min-p classifier')

        ## MLP model training & assessment
        solve(train_sets, test_sets)

    if OBSERVE:
        # Plot results
        plot_samples(train_sets[100], 'train set 100')
        plot_samples(train_sets[200], 'train set 200')
        plot_samples(train_sets[500], 'train set 500')
        plot_samples(train_sets[1000], 'train set 1k')
        plot_samples(train_sets[2000], 'train set 2k')
        plot_samples(train_sets[5000], 'train set 5k')
        plot_samples(test_sets[100000], 'test set 100k')
        plot_error_vs_size()
        plot_error_vs_perceptron_no() 
        # plot_optimal_perceptron_no_vs_size()

main()
