# Question 1 HW3
# Author: Andrew Nedea

# 4 classes with uniform priors, Gaussian pdfs
import numpy as np
from numpy.linalg import eig
from scipy.stats import multivariate_normal
from collections import defaultdict
import matplotlib.pyplot as plt
from dataclasses import dataclass
import random

# Eigenvalues of covariance matrices should be in range [0.5, 1.4]
cov_eigen_bound = [0.5, 1.4]

# Set sizes
train_set_sizes = [100, 200, 500, 1000, 2000, 5000]
test_set_sizes = [100000]

@dataclass
class LabeledBox:
    label: str
    value: object

# Class parameters
classes = {
        1: { 
            "mean": [1, 1, 1],
            "cov": [[ 0.8, 0.03, 0.02 ],
                    [ 0.03, 1.12, 0.03],
                    [ 0.02, 0.03, 1.06]],
            "color": "green"
        },
        2: { 
            "mean": [1, -1, 1],
            "cov": [[ 0.92, 0.01, 0.09 ],
                    [ 0.01, 1.32, 0.03],
                    [ 0.09, 0.03, 0.66]],
            "color": "red"
        },
        3: { 
            "mean": [-1, -1, 1],
            "cov": [[ 1.12, 0.02, 0.19 ],
                    [ 0.02, 1.11, 0.03],
                    [ 0.19, 0.03, 0.71]],
            "color": "blue"
        },
        4: { 
            "mean": [-1, 1, -1],
            "cov": [[ 1.02, 0.03, 0.24 ],
                    [ 0.03, 0.97, 0.2 ],
                    [ 0.24, 0.2, 0.82]],
            "color": "purple"
        },
};

uniform_class_prior = 1.0 / len(classes)

def generate_and_sample(sample_no) -> dict:
    """
    Generates distributions and samples for each class and returns a dictionary 
    that maps class labels to a child dictionary. The child dictionary contains 
    two keys (samples and dist). The "samples" key maps to the vector of samples
    X for that class and the "dist" key maps to the multivariate_normal object for
    the class.

    i.e.
    {
        1: {"samples": [[12, 34, 42], ... [123, 2, 3]], "dist": object},
        2: ..
    }

    :param sample_no: number of samples to generate
    """
    results = {}

    sample_no_per_class = int(sample_no / len(classes))

    print(f'Generating {sample_no_per_class} samples per class')

    for label, params in classes.items():
        dist = multivariate_normal(params["mean"], params["cov"])
        x = dist.rvs(size=sample_no_per_class)

        results[label] = {"samples": x, "dist": dist}

    return results

def covariance_contract() -> None:
    """
    Ensures that the covariance contract is respected (that the eigenvalues of the
    covariance matrices are within cov_eigen_bound).
    """
    for label, params in classes.items():
        if not all([(cov_eigen_bound[0] <= eigenv <= cov_eigen_bound[1]) \
                for eigenv in eig(params["cov"])[0]]):
            raise Exception('At least one covariance matrix has eigenvalues outside interval!')
        print(eig(params["cov"])[0])


def plot_samples(class_dist_samples: dict) -> None:
    """
    Plots the sample points generated by generate_and_sample.

    :param class_dist_samples: dictionary with class labels for keys and child dictionary containing
                               "samples" for values
    """
    fig = plt.figure(0)
    ax = fig.add_subplot(1, 1, 1, projection='3d')

    for class_label, data in class_dist_samples.items():
        samples = data["samples"]
        ax.set_xlabel('x0')
        ax.set_ylabel('x1')
        ax.set_zlabel('x2')
        img = ax.scatter(samples[:, 0], samples[:, 1], samples[:, 2],
                c=classes[class_label]["color"])

    #plt.show()

covariance_contract()

def generate_sets() -> (dict, dict):
    """
    Generates training and testing sets for each size specified in
    train_set_sizes and test_set_sizes, respectively.

    For each type of set (train & test), a dictionary is created indexed
    by set size with value equal to the dict returned by generate_and_sample for
    that size.

    example of object returned:
           
    tuple(
          v----- training set of size 100 for each class (1, 2 etc.)
        {100: {1: {"samples": [[..]], "dist": }, 2: {"samples: [[..]], "dist": }, ..},
        {10000: {1: {"samples: [..], "dict": ..}, 2: {"samples":...}}}
         ^--- test set of size 10000 for each class (1, 2 etc.)
    )

    :return: a tuple made up of training set dict and test set dict
    """
    # maps from set size to dict returned by generate_and_sample of that size
    train_sets = {}
    test_sets = {}

    print('Generating sets..')

    for size in train_set_sizes:
        train_sets[size] = generate_and_sample(size)
        print(f' -> Generated training set of size {size}')

    for size in test_set_sizes:
        test_sets[size] = generate_and_sample(size)
        print(f' -> Generated test set of size {size}')

    plot_samples(train_sets[5000])

    return train_sets, test_sets

def minp_classification(class_dist_samples: dict):
    """
    Takes in a dictionary that maps a class label to samples & the distribution
    the samples came from. It then performs min-p classification to determine
    the minimum probability of error.

    :param class_dist_samples: dict returned by generate_and_sample
    """
    labels = list(class_dist_samples.keys())
    incorrectly_labeled_no = 0

    # maps class label to distributions
    distributions = {}
    for label, items in class_dist_samples.items():
        distributions[label] = items["dist"]

    # predicted label -> (true_label, sample)
    classifications = defaultdict(list)
    counter = 0
    print('Total sample no.: ', len(class_dist_samples) * len(class_dist_samples[1]['samples']))
    # for each class
    for true_label, items in class_dist_samples.items():
        # for each sample of that class
        for sample in items["samples"]:
            best_risk = 9999999
            best_label = 0
            # for each class type compute risk
            for label in labels:
                risk = 0
                # for every other class label
                for label2 in labels:
                    if label != label2:
                        # compute risk by adding posteriors
                        risk += distributions[label2].pdf(sample) * uniform_class_prior

                #  check if risk for this decision is better (lower) than running risk
                if risk < best_risk:
                    best_risk = risk
                    best_label = label

            classifications[best_label].append((true_label, sample))

            counter += 1

            if counter % 1000 == 0:
                print('Counter: ', counter)


    # for each class / label and samples classified as such
    for classification, labeled_samples in classifications.items():
        # for each sample classified as 'classification'
        for (true_label, sample) in labeled_samples:
            # check if incorrectly classified
            if true_label != classification:
                #print(f'Correct label {true_label} Classified as {classification}')
                incorrectly_labeled_no += 1

    print('Incorrectly classified no.: ', incorrectly_labeled_no)
    print('P(error)', incorrectly_labeled_no/counter)

    # min prob of error is no of incorrect decisions / total no of samples
train_sets, test_sets = generate_sets()
minp_classification(test_sets[100000])
print('uniform prior: ', uniform_class_prior)
