# Question 1 HW3
# Author: Andrew Nedea

# 4 classes with uniform priors, Gaussian pdfs
import numpy as np
from numpy.linalg import eig
from scipy.stats import multivariate_normal
from collections import defaultdict
import matplotlib.pyplot as plt
from keras import models
from keras import layers
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
import tensorflow as tf
import random
import logging
from sklearn.metrics import accuracy_score

tf.get_logger().setLevel(logging.ERROR)

# Eigenvalues of covariance matrices should be in range [0.5, 1.4]
cov_eigen_bound = [0.5, 1.4]

# Set sizes
train_set_sizes = [100, 200, 500, 1000, 2000, 5000]
test_set_sizes = [100000]

feature_no = 3

# Class parameters
classes = {
        1: { 
            "mean": [1, 1, 1],
            "cov": [[ 0.8, 0.03, 0.02 ],
                    [ 0.03, 1.12, 0.03],
                    [ 0.02, 0.03, 1.06]],
            "color": "green",
            "marker": "o"
        },
        2: { 
            "mean": [1, -1, 1],
            "cov": [[ 0.92, 0.01, 0.09 ],
                    [ 0.01, 1.32, 0.03],
                    [ 0.09, 0.03, 0.66]],
            "color": "red",
            "marker": "^",
        },
        3: { 
            "mean": [-1, -1, 1],
            "cov": [[ 1.12, 0.02, 0.19 ],
                    [ 0.02, 1.11, 0.03],
                    [ 0.19, 0.03, 0.71]],
            "color": "blue",
            "marker": "s"
        },
        4: { 
            "mean": [-1, 1, -1],
            "cov": [[ 1.02, 0.03, 0.24 ],
                    [ 0.03, 0.97, 0.2 ],
                    [ 0.24, 0.2, 0.82]],
            "color": "purple",
            "marker": "D"
        },
};

uniform_class_prior = 1.0 / len(classes)

def generate_and_sample(sample_no) -> dict:
    """
    Generates distributions and samples for each class and returns a dictionary 
    that maps class labels to a child dictionary. The child dictionary contains 
    two keys (samples and dist). The "samples" key maps to the vector of samples
    X for that class and the "dist" key maps to the multivariate_normal object for
    the class.

    i.e.
    {
        1: {"samples": [[12, 34, 42], ... [123, 2, 3]], "dist": object},
        2: ..
    }

    :param sample_no: number of samples to generate
    """
    results = {}

    sample_no_per_class = int(sample_no / len(classes))

    print(f'Generating {sample_no_per_class} samples per class')

    for label, params in classes.items():
        dist = multivariate_normal(params["mean"], params["cov"])
        x = dist.rvs(size=sample_no_per_class)

        results[label] = {"samples": x, "dist": dist}

    return results

def covariance_contract() -> None:
    """
    Ensures that the covariance contract is respected (that the eigenvalues of the
    covariance matrices are within cov_eigen_bound).
    """
    for label, params in classes.items():
        if not all([(cov_eigen_bound[0] <= eigenv <= cov_eigen_bound[1]) \
                for eigenv in eig(params["cov"])[0]]):
            raise Exception('At least one covariance matrix has eigenvalues outside interval!')
        print(eig(params["cov"])[0])


def plot_samples(class_dist_samples: dict) -> None:
    """
    Plots the sample points generated by generate_and_sample.

    :param class_dist_samples: dictionary with class labels for keys and child dictionary containing
                               "samples" for values
    """
    fig = plt.figure(0)
    ax = fig.add_subplot(1, 1, 1, projection='3d')

    handles = []
    for class_label, data in class_dist_samples.items():
        samples = data["samples"]
        ax.set_xlabel('x0')
        ax.set_ylabel('x1')
        ax.set_zlabel('x2')
        img = ax.scatter(samples[:, 0], samples[:, 1], samples[:, 2],
                c=classes[class_label]["color"], label=f'Class {class_label}')
        handles.append(img)

    plt.legend(handles=handles)
    #plt.show()

covariance_contract()

def generate_sets() -> (dict, dict):
    """
    Generates training and testing sets for each size specified in
    train_set_sizes and test_set_sizes, respectively.

    For each type of set (train & test), a dictionary is created indexed
    by set size with value equal to the dict returned by generate_and_sample for
    that size.

    example of object returned:
           
    tuple(
          v----- training set of size 100 for each class (1, 2 etc.)
        {100: {1: {"samples": [[..]], "dist": }, 2: {"samples: [[..]], "dist": }, ..},
        {10000: {1: {"samples: [..], "dict": ..}, 2: {"samples":...}}}
         ^--- test set of size 10000 for each class (1, 2 etc.)
    )

    :return: a tuple made up of training set dict and test set dict
    """
    # maps from set size to dict returned by generate_and_sample of that size
    train_sets = {}
    test_sets = {}

    print('Generating sets..')

    for size in train_set_sizes:
        train_sets[size] = generate_and_sample(size)
        print(f' -> Generated training set of size {size}')

    for size in test_set_sizes:
        test_sets[size] = generate_and_sample(size)
        print(f' -> Generated test set of size {size}')

    plot_samples(train_sets[5000])

    return train_sets, test_sets

def minp_classification(class_dist_samples: dict):
    """
    Takes in a dictionary that maps a class label to samples & the distribution
    the samples came from. It then performs min-p classification to determine
    the minimum probability of error.

    :param class_dist_samples: dict returned by generate_and_sample
    """
    labels = list(class_dist_samples.keys())
    incorrectly_labeled_no = 0

    # maps class label to distributions
    distributions = {}
    for label, items in class_dist_samples.items():
        distributions[label] = items["dist"]

    # predicted label -> (true_label, sample)
    classifications = defaultdict(list)
    counter = 0
    print('Total sample no.: ', len(class_dist_samples) * len(class_dist_samples[1]['samples']))
    # for each class
    for true_label, items in class_dist_samples.items():
        # for each sample of that class
        for sample in items["samples"]:
            best_prob = -999999
            best_label = 0
            # for each class type determine posterior
            for label in labels:
                prob = distributions[label].pdf(sample) * uniform_class_prior
                
                # check if this risk beats previous
                if best_prob < prob:
                    best_prob = prob
                    best_label = label

            classifications[best_label].append((true_label, sample))

            # increment counter
            counter += 1

            # print where we are
            if counter % 1000 == 0:
                print('Counter: ', counter)


    # correctly classified samples
    cc = []
    # incorrectly classified samples
    icc = []
    # for each class / label and samples classified as such
    for classification, labeled_samples in classifications.items():
        # for each sample classified as 'classification'
        for (true_label, sample) in labeled_samples:
            # check if incorrectly classified
            if true_label != classification:
                #print(f'Correct label {true_label} Classified as {classification}')
                incorrectly_labeled_no += 1
                icc.append((true_label, sample))
            else:
                cc.append((true_label, sample))

    # class label
    print('Incorrectly classified no.: ', incorrectly_labeled_no)
    print('P(error)', incorrectly_labeled_no/counter)

    return cc, icc

def plot_classification(cc, icc):
    """
    Plots correctly and incorrectly classified samples.

    :param cc: list of correctly classified sample tuples (true_label, sample)
    :param icc: list of incorrectly classified sample tuples (true_label, sample)
    """
    # dict to hold correctly classified samples by class
    ccl = defaultdict(list)
    # dict to hold incorrectly classified samples by class
    iccl = defaultdict(list)

    # convert to dicts indexed by true class label
    for true_label, sample in cc:
        ccl[true_label].append(sample)

    for true_label, sample in icc:
        iccl[true_label].append(sample)

    fig = plt.figure(1)
    ax = fig.add_subplot(1, 1, 1, projection='3d')
    ax.title.set_text('Classification result')
    ax.set_xlabel('x0')
    ax.set_ylabel('x1')
    ax.set_zlabel('x2')

    handles = []

    for k in ccl.keys():
        # convert each entry to an np array for slicing
        ccl[k] = np.array(ccl[k])
        iccl[k] = np.array(iccl[k])

        handles.append(ax.scatter(ccl[k][:, 0], ccl[k][:, 1], ccl[k][:, 2],
            marker=classes[k]['marker'], color='green',
            label=f'Class {k} correctly classified'))

        handles.append(ax.scatter(iccl[k][:, 0], iccl[k][:, 1], iccl[k][:, 2],
            marker=classes[k]['marker'], color='red',
            label=f'Class {k} incorrectly classified'))

    plt.legend(handles=handles)
    plt.show()

def create_nn(perceptron_no: int):
    """
    Creates a multi-layer perceptrons with the specified number of perceptrons
    for its first layer. Its second layer uses a softmax activation function to fit
    results within [0, 1].

    :param perceptron_no: number of perceptrons:
    :return: compiled model
    """
    net = models.Sequential()
    # add hidden layer
    net.add(layers.Dense(units=perceptron_no, activation='elu', input_shape=(feature_no,)))
    # add output layer
    net.add(layers.Dense(units=len(classes), activation='softmax'))
    # compile
    net.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])

    return net

def create_classifier(perceptron_no, epoch_no=500):
    nn = KerasClassifier(build_fn=create_nn, perceptron_no=perceptron_no, epochs=epoch_no, batch_size=100, verbose=0)

    return nn

def run_cross_val(X, Y, perceptron_no, epoch_no):
    """
    Runs 10-fold cross-validation on given training data.

    :param X: data
    :param Y: labels
    :param perceptron_no: number of perceptrons of NN
    :param epoch_no: number of epochs to train on
    :return: mean accuracy obtained from the k-folds
    """
    print(f'Sample no: {len(Y)} {len(X)}')

    nn = create_classifier(perceptron_no, epoch_no)
    scores = cross_val_score(nn, X, Y, cv=10)

    return np.mean(scores)

def splice(set_: dict) -> (list, list):
    """
    Processes data for training.

    :param set_: dict indexed by class label that maps to a dict with two fields
                 (samples and distr), as returned by generate_sets

    :return: tuple containing the features and their corresponding labels
    """
    X = []
    Y = []

    for label, items in set_.items():
        for sample in items["samples"]:
            X.append(np.array(sample))
            Y.append(label)

    X = np.array(X)
    Y = np.array(Y)

    # one-hot encode label
    Y = Y.reshape(len(Y), 1)
    onehot_encoder = OneHotEncoder(sparse=False)
    Y = onehot_encoder.fit_transform(Y)

    return X, Y

# min prob of error is no of incorrect decisions / total no of samples
train_sets, test_sets = generate_sets()
## Min-p classification
# cc, icc = minp_classification(test_sets[100000])
# plot_classification(cc, icc)

# ANN stuff

def find_best_topology():
    # maps set size -> optimal perceptron no.
    size_to_optimal_p = {}
    # maps set size -> pair of (X, Y)
    size_to_XY = {}
    epoch_no = 500

    # Run cross-validation to determine best topology (no. of perceptrons)
    # for each set size
    for size, set_ in train_sets.items():
        print(f'=== Training set of size {size} ===')
        best_error = 9999
        best_perceptron_no = 0

        # get training pair
        X, Y = splice(set_)
        # cache training pair
        size_to_XY[size] = (X, Y)

        # try out different no. of perceptrons
        for k in range(1, 4):
            # get mean accuracy from cross-validation
            acc = run_cross_val(X, Y, k, epoch_no)
            # compute error
            error = 1 - acc

            print(f'Perceptron no. = {k} Avg cross-validation acc: {acc}')

            # check if this topology beats the running best error-wise
            if error < best_error:
                best_error = error
                best_perceptron_no = k

        print(f'Optimal no. of perceptrons for set size {size}: {best_perceptron_no}')
        # save best topology (no. of perceptrons) for this size set
        size_to_optimal_p[size] = best_perceptron_no
        print('\n')

    # Reconstruct optimal models & train them on (whole) training set pertaining to their size
    size_to_model = {}

    for size, optimal_p in size_to_optimal_p.items():
        # rebuild model with same no. of perceptrons it achieved best accuracy on
        size_to_model[size] = create_classifier(optimal_p)
        # train model
        size_to_model[size].fit(size_to_XY[size][0], size_to_XY[size][1])

    return size_to_model

def solve():
    """
    Finds the best topology (no. of perceptrons) for each training set by
    picking the ones with the lowest error / highest accuracy. It then runs
    each trained optimal model (each model corresponding to a training set size) 
    on the test set and reports the errors.
    """
    size_to_model = find_best_topology()
    X_test, Y_test = splice(test_sets[100000])

    for size, model in size_to_model.items():
        # run on test set
        prediction = model.predict_proba(X_test)

        incorrectly_classified_no = 0
        error = 0

        for k in range(len(Y_test)):
            predicted_posteriors = prediction[k]
            # retrieve index of predicted class label by picking the maximum poster
            predicted_index = np.argmax(predicted_posteriors)

            # retrieve index of actual label
            actual_index = np.argmax(Y_test[k])
            # check if mismatch between predicted label & actual label
            if predicted_index != actual_index:
                incorrectly_classified_no += 1

        # determine error for this MLP model
        error = incorrectly_classified_no / len(Y_test)

        print(f'Min. prob of error (Ntrain = {size}) = {error}')

solve()
